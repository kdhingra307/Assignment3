{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tuple_to_dict:\n",
    "    \n",
    "    def __init__(self, tuple_object):\n",
    "        self.tuple = tuple_object\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tuple)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            \"input\" : self.tuple[index][0],\n",
    "            \"target\" : self.tuple[index][1],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist(train=False):\n",
    "    from torchvision import datasets, transforms\n",
    "    return tuple_to_dict(datasets.MNIST(\"./resources/data/raw\", train=train, transform=transforms.Compose([transforms.Grayscale(num_output_channels=3),transforms.Resize([32, 32]), transforms.ToTensor()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mnist_m:\n",
    "    \n",
    "    def __init__(self, path, train=False):\n",
    "        \n",
    "        if train:\n",
    "            self.files = open(path+\"/mnist_m_train_labels.txt\").read().split(\"\\n\")[:-1]\n",
    "            self.path = path + \"/mnist_m_train/\"\n",
    "        else:\n",
    "            self.files = open(path+\"/mnist_m_test_labels.txt\").read().split(\"\\n\")[:-1]\n",
    "            self.path = path + \"/mnist_m_test/\"\n",
    "        \n",
    "        print(\"Fetching {} files\".format(len(self.files)))\n",
    "        \n",
    "        self.data = []\n",
    "        for e in self.files:\n",
    "            ee = e.split(\" \")\n",
    "            self.data.append({\n",
    "                \"input\" : ee[0],\n",
    "                \"target\" : int(ee[1])\n",
    "            })\n",
    "        \n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.path + self.data[index]['input'])\n",
    "        \n",
    "        return {\n",
    "            \"input\" : self.transform(image),\n",
    "            \"target\" : self.data[index]['target']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 59001 files\n",
      "Fetching 9001 files\n"
     ]
    }
   ],
   "source": [
    "dataset = {\n",
    "    \"train\" :{\n",
    "        \"mnist\" : torch.utils.data.DataLoader(get_mnist(train=True), batch_size=64),\n",
    "        \"mnist_m\" : torch.utils.data.DataLoader(mnist_m(\"resources/data/raw/mnist_m/\", train=True), batch_size=64),\n",
    "    },\n",
    "    \"test\" : {\n",
    "        \"mnist\" : torch.utils.data.DataLoader(get_mnist(train=False), batch_size=64),\n",
    "        \"mnist_m\" : torch.utils.data.DataLoader(mnist_m(\"resources/data/raw/mnist_m/\", train=False), batch_size=64),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class grl_grad(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return input\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return -1*grad_output\n",
    "\n",
    "\n",
    "\n",
    "class grl(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(grl, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        grl_grad.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(torch.nn.Module):        \n",
    "    \n",
    "    def __encoder__(self):\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(kernel_size=5, in_channels=3, out_channels=32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Conv2d(kernel_size=5, in_channels=32, out_channels=48),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "    \n",
    "    def __classifier__(self):\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.encoder_shape, out_features=100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=100, out_features=100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=100, out_features=10),\n",
    "        )\n",
    "    \n",
    "    def __domain__(self):\n",
    "        self.domain =  torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.encoder_shape, out_features=100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=100, out_features=1),\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def __init__(self,):\n",
    "        super(model, self).__init__()\n",
    "        self.encoder_shape = 48*5*5   \n",
    "        self.__encoder__()\n",
    "        self.__classifier__()\n",
    "        self.grl = grl()\n",
    "        self.__domain__()\n",
    "    \n",
    "    \n",
    "    def forward(self, x, split=False):\n",
    "        encoded = self.encoder(x).reshape([-1, self.encoder_shape])\n",
    "        batch_size = encoded.shape[0]\n",
    "        start_index = 0\n",
    "        end_index = batch_size// 2\n",
    "        \n",
    "        if split:\n",
    "            start_index = end_index\n",
    "            end_index= batch_size\n",
    "        \n",
    "        return {\n",
    "                \"classifier\" : self.classifier(encoded[start_index:end_index]),\n",
    "                \"domain\" : self.domain(encoded).squeeze(-1)\n",
    "            }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_stats(stats, model, step):\n",
    "    tensorboard_writer.add_scalar(\"loss/epoch/{}/domain\".format(model), np.mean(loss['loss']['domain']), step)\n",
    "    tensorboard_writer.add_scalar(\"loss/epoch/{}/classifier\".format(model), np.mean(loss['loss']['classifier']), step)\n",
    "    tensorboard_writer.add_scalar(\"acc/epoch/{}/domain\".format(model), np.mean(loss['acc']['domain']), step)\n",
    "    tensorboard_writer.add_scalar(\"acc/epoch/{}/classifier\".format(model), np.mean(loss['acc']['classifier']), step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train:\n",
    "    \n",
    "    def __init__(self, model_to_train):\n",
    "        self.model = model_to_train()\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "        self.model.to(self.device)\n",
    "        self.loss = {\n",
    "            \"logistic\" : torch.nn.BCEWithLogitsLoss(),\n",
    "            \"softmax\" : torch.nn.CrossEntropyLoss(),\n",
    "        }\n",
    "        self.optimizer = torch.optim.SGD(lr=0.01, momentum=0.9, params=self.model.parameters())\n",
    "        self.__get_tensorboard()\n",
    "    \n",
    "    def loop(self, X, to_cpu=False):\n",
    "        X = X.to(self.device)\n",
    "        if to_cpu:\n",
    "            return self.model(X).cpu()    \n",
    "        return self.model(X)\n",
    "    \n",
    "    def compute_loss(self, X, output):\n",
    "        print(X['target'].keys(), output.keys())\n",
    "        loss = self.loss['softmax'](target=X['target']['classifier'], input=output['classifier'])\n",
    "        loss += self.loss['logistic'](target=X['target']['domain'], input=output['domain'])\n",
    "        return loss\n",
    "    \n",
    "    def stats(self, X):\n",
    "        output = self.loop(X['input'])\n",
    "        loss = self.compute_loss(X, output)\n",
    "        classifier_acc = np.count_nonzero(torch.nn.softmax(output['classifier']).argmax(-1) == X['target']['classifier'])/len(X['target']['classifier'])\n",
    "        target_out = self.model(X['input'], split=False)\n",
    "        domain_acc = np.count_nonzero(torch.nn.softmax(target_out['classifier']).argmax(-1) == X['target']['domain_classifier'])/len(X['target']['domain_classifier'])\n",
    "        \n",
    "        return {\n",
    "            \"accuracy\" : {\n",
    "                \"classifier\" : classifier_acc,\n",
    "                \"domain\" : domain_acc,\n",
    "            },\n",
    "            \"loss\" : loss\n",
    "        }\n",
    "    \n",
    "    def cycle(self, X):\n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.loop(X['input'])\n",
    "        cur_loss = self.compute_loss(X, output)\n",
    "        cur_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return cur_loss\n",
    "    \n",
    "    def batch(self, mnist_batch, mnist_m_batch, task):\n",
    "        \n",
    "        mnist_batch_size = mnist_batch['input'].shape[0]\n",
    "        mnist_m_batch_size = mnist_m_batch['input'].shape[0]\n",
    "        \n",
    "        X = {\n",
    "            \"input\" : torch.cat([mnist_batch['input'], mnist_m_batch['input']], axis=0).to(self.device),\n",
    "            \"target\" : {\n",
    "                \"classifier\" : mnist_batch['target'].to(self.device),\n",
    "                \"domain\" : torch.cat([torch.ones(mnist_batch_size), torch.zeros(mnist_m_batch_size)], axis=0).to(self.device),\n",
    "                \"domain_classifier\" : mnist_m_batch['target'].to(self.device)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if task:\n",
    "            return trainer.cycle(X)\n",
    "        else:\n",
    "            return train.stats(X)\n",
    "    \n",
    "    def __get_tensorboard(self):\n",
    "        from torch.utils.tensorboard import SummaryWriter\n",
    "        from datetime import datetime\n",
    "\n",
    "        task_type = \"MNIST\"\n",
    "        time_date = \"{}\".format(datetime.now())\n",
    "        self.tensorboard_writer = SummaryWriter(\"log/{}/{}\".format(task_type, time_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1276d818da6a4dd2bf15d30aee4882bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['classifier', 'domain', 'domain_classifier']) dict_keys(['classifier', 'domain'])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tensorboard_writer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-174-0d98e2af115b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmnist_m_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_m_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnist_m_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtensorboard_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss/batch/train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     stats = {\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tensorboard_writer' is not defined"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "for epoch in tqdm(range(1000)):\n",
    "    \n",
    "    mnist_m_iter = dataset['train']['mnist_m'].__iter__()\n",
    "    for mnist_batch in dataset['train']['mnist']:\n",
    "        \n",
    "        mnist_m_batch = next(mnist_m_iter)\n",
    "        loss = trainer.batch(mnist_batch, mnist_m_batch, True)\n",
    "        trainer.tensorboard_writer.add_scalar(\"loss/batch/train\", loss.item())\n",
    "    \n",
    "    stats = {\n",
    "        \"loss\":{\n",
    "            \"domain\":[],\n",
    "            \"classifer\":[],\n",
    "        },\n",
    "        \"acc\" : {\n",
    "            \"domain\":[],\n",
    "            \"classifier\":[]\n",
    "        }\n",
    "    }\n",
    "    mnist_m_iter = dataset['train']['mnist_m'].__iter__()\n",
    "    for mnist_batch in dataset['train']['mnist']:\n",
    "        \n",
    "        mnist_m_batch = next(mnist_m_iter)\n",
    "        batch_stats = trainer.batch(mnist_batch, mnist_m_batch, False)\n",
    "        stats['loss']['domain'].append(batch_stats['loss']['domain'])\n",
    "        stats['loss']['classifier'].append(batch_stats['loss']['classifier'])\n",
    "        stats['acc']['domain'].append(batch_stats['acc']['domain'])\n",
    "        stats['acc']['classifier'].append(batch_stats['acc']['classifier'])\n",
    "    \n",
    "    write_stats(stats, \"train\", epoch)\n",
    "    \n",
    "    stats = {\n",
    "        \"loss\":{\n",
    "            \"domain\":[],\n",
    "            \"classifer\":[],\n",
    "        },\n",
    "        \"acc\" : {\n",
    "            \"domain\":[],\n",
    "            \"classifier\":[]\n",
    "        }\n",
    "    }\n",
    "    mnist_m_iter = dataset['test']['mnist_m'].__iter__()\n",
    "    for mnist_batch in dataset['test']['mnist']:\n",
    "        \n",
    "        mnist_m_batch = next(mnist_m_iter)\n",
    "        batch_stats = trainer.batch(mnist_batch, mnist_m_batch, False)\n",
    "        stats['loss']['domain'].append(batch_stats['loss']['domain'])\n",
    "        stats['loss']['classifier'].append(batch_stats['loss']['classifier'])\n",
    "        stats['acc']['domain'].append(batch_stats['acc']['domain'])\n",
    "        stats['acc']['classifier'].append(batch_stats['acc']['classifier'])\n",
    "    \n",
    "    write_stats(stats, \"test\", epoch)\n",
    "    \n",
    "    \n",
    "    if np.mean(stats['acc']['domain']) > best_acc:\n",
    "        torch.save(segnet_model.state_dict(), \"./weights/q3_{}\".format(epoch))\n",
    "        \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
